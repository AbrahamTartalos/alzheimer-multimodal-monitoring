{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78213a8f-541c-46b0-a0ec-3e74d0a2eb0a",
   "metadata": {},
   "source": [
    "# 04c - Classification Models Development\n",
    "\n",
    "**Objetivo**: Desarrollar modelos de clasificación para predecir categorías de riesgo de Alzheimer (Low, Moderate, High)\n",
    " \n",
    "**Target Variable**: `risk_category`\n",
    "**Clases**: Low (46.4%), Moderate (46.1%), High (7.5%)\n",
    " \n",
    "**Modelos a desarrollar**:\n",
    "- Logistic Regression (baseline)\n",
    "- Random Forest Classifier\n",
    "- Gradient Boosting (XGBoost, LightGBM)\n",
    "- Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57a538-e28c-4bea-9296-af88da7f04a9",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d97ff-efaf-42b9-a69b-ae4d668618ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8d75a-1da4-4855-9037-dc0b02ed5b3e",
   "metadata": {},
   "source": [
    "## Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd9af2a-45b3-4304-95cc-4af16168689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src/modeling')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import StackingClassifier  # Import explícito por si acaso\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import model_utils\n",
    "# Importar scripts personalizados\n",
    "from classification_pipeline import ClassificationPipeline\n",
    "from ensemble_methods import AlzheimerEnsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e86347b-8ef9-438a-9dc8-437ce9dfec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440d9460-f3d5-4ad3-b33d-2161a3d30be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65d8716f-36d0-4aa3-ae3c-5718b035b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías y scripts importados correctamente\n",
      "📅 Fecha de ejecución: 2025-06-22 18:18:38\n"
     ]
    }
   ],
   "source": [
    "print(\"✅ Librerías y scripts importados correctamente\")\n",
    "print(f\"📅 Fecha de ejecución: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b90eb8-4fe0-46cc-a799-74da4481638b",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bedef8b-239b-43c0-9b8d-9a6095dfc3fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Dataset cargado: (48466, 186)\n",
      "🎯 Distribución de clases:\n",
      "risk_category\n",
      "Low         22501\n",
      "Moderate    22345\n",
      "High         3620\n",
      "Name: count, dtype: int64\n",
      "📊 Porcentajes:\n",
      "risk_category\n",
      "Low         46.4\n",
      "Moderate    46.1\n",
      "High         7.5\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos procesados\n",
    "try:\n",
    "    df = pd.read_csv('../data/processed/features/alzheimer_features_selected_20250621.csv')\n",
    "    print(f\"📊 Dataset cargado: {df.shape}\")\n",
    "    \n",
    "    # Verificar target variable\n",
    "    if 'risk_category' in df.columns:\n",
    "        print(f\"🎯 Distribución de clases:\")\n",
    "        class_dist = df['risk_category'].value_counts()\n",
    "        print(class_dist)\n",
    "        print(f\"📊 Porcentajes:\")\n",
    "        print((class_dist / len(df) * 100).round(1))\n",
    "    else:\n",
    "        print(\"❌ Error: Variable target 'risk_category' no encontrada\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Error: Archivo de features no encontrado\")\n",
    "    print(\"💡 Ejecuta primero el notebook 03_feature_engineering_master.ipynb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74a4b5-c826-4617-bdd0-24a4230551f4",
   "metadata": {},
   "source": [
    "## Paso 1: Purga de Features con Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cd2c2d-e42c-4a84-9a08-42d277b37f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features eliminadas: ['composite_risk_score', 'diagnosis_code', 'CDRSB_percentile', 'CDRSB_LOG', 'demographic_risk_score']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PASO 1: Purga de features con leakage (VERSIÓN MEJORADA)\n",
    "# =============================================================================\n",
    "\n",
    "leakage_features = [\n",
    "    'risk_category_num',\n",
    "    'composite_risk_score',\n",
    "    'diagnosis_code',\n",
    "    'CDRSB_percentile',\n",
    "    'CDRSB_LOG',\n",
    "    'demographic_risk_score',  # Nueva adición\n",
    "    'multimodal_risk_index'    # Nueva adición\n",
    "]\n",
    "\n",
    "# Verificar existencia antes de eliminar\n",
    "existing_leakage = [f for f in leakage_features if f in df.columns]\n",
    "df_clean = df.drop(columns=existing_leakage, errors='ignore')\n",
    "\n",
    "print(f\"✅ Features eliminadas: {existing_leakage}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036ede91-3d62-49f9-891f-3ee6c8ce4732",
   "metadata": {},
   "source": [
    "## Paso 2: Reingeniería de Features Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b99b1e-7431-4cd0-aa83-7012a1c404c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función mejorada para cambios temporales (sin cambios)\n",
    "def create_safe_cdrsb_change(df):\n",
    "    df = df.sort_values(['RID', 'DAYS_SINCE_BASELINE'])\n",
    "    df['DAYS_SINCE_LAST_VISIT'] = df.groupby('RID')['DAYS_SINCE_BASELINE'].diff()\n",
    "    df['CDRSB_PREV'] = df.groupby('RID')['CDRSB'].shift(1)\n",
    "    df['CDRSB_CHANGE_SAFE'] = np.where(\n",
    "        df['DAYS_SINCE_LAST_VISIT'] >= 90,\n",
    "        df['CDRSB'] - df['CDRSB_PREV'],\n",
    "        np.nan\n",
    "    )\n",
    "    return df.drop(columns=['CDRSB_PREV', 'DAYS_SINCE_LAST_VISIT'])\n",
    "\n",
    "df_clean = create_safe_cdrsb_change(df_clean)\n",
    "\n",
    "# Corregido: Usar columna existente (ej: sleep_efficiency_mean)\n",
    "df_clean['sleep_quality_trend'] = df_clean.groupby('RID')['sleep_efficiency_mean'].transform(\n",
    "    lambda x: x.rolling(window=3, min_periods=2).mean()\n",
    ")\n",
    "\n",
    "# Imputar columnas\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "sleep_cols = ['sleep_minutes_mean', 'sleep_efficiency_mean', 'sleep_disruptions_mean']  # Columnas reales\n",
    "df_clean[sleep_cols] = imputer.fit_transform(df_clean[sleep_cols])\n",
    "\n",
    "# Bandera + imputación para CDRSB (sin cambios)\n",
    "df_clean['CDRSB_MISSING'] = df_clean['CDRSB'].isna().astype(int)\n",
    "df_clean['CDRSB'] = df_clean.groupby('RID')['CDRSB'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5011ec8c-dfa7-4c92-a82c-b1dd9dfe4dcb",
   "metadata": {},
   "source": [
    "## Ejecutar Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "247bcf03-8633-4f1d-9154-f70106e06ebb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Ejecutando pipeline completo de clasificación...\n",
      "🚀 Iniciando pipeline de clasificación...\n",
      "📊 Datos preparados: 38772 train, 9694 test\n",
      "Entrenando logistic_regression...\n",
      "Entrenando random_forest...\n",
      "Entrenando gradient_boosting...\n",
      "Entrenando svm...\n",
      "   Usando subconjunto de 10000 muestras para SVM\n",
      "✅ 4 modelos entrenados\n",
      "📈 Evaluación completada\n",
      "Validación cruzada para logistic_regression...\n",
      "Validación cruzada para random_forest...\n",
      "Validación cruzada para gradient_boosting...\n",
      "Validación cruzada para svm...\n",
      "🔄 Validación cruzada completada\n",
      "🏆 Mejor modelo: gradient_boosting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/03 18:43:50 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/03 18:45:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Resultados registrados en MLflow\n",
      "🏆 Mejor modelo seleccionado: gradient_boosting\n"
     ]
    }
   ],
   "source": [
    "# Inicializar pipeline de clasificación\n",
    "classification_pipeline = ClassificationPipeline()\n",
    "\n",
    "# Ejecutar el pipeline completo\n",
    "print(\"🚀 Ejecutando pipeline completo de clasificación...\")\n",
    "pipeline_results = classification_pipeline.run_pipeline(df_clean, target_col='risk_category')\n",
    "\n",
    "# Obtener resultados\n",
    "results = pipeline_results['results']\n",
    "cv_results = pipeline_results['cv_results']\n",
    "trained_models = pipeline_results['trained_models']\n",
    "best_model_name = pipeline_results['best_model']\n",
    "\n",
    "print(f\"🏆 Mejor modelo seleccionado: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06479757-02f1-4569-8f2a-03601d034bfc",
   "metadata": {},
   "source": [
    "## Paso 3: Entrenamiento con Validación Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf200f44-1960-462c-9984-b2a46f0709f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Rendimiento en Test Temporal:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      1.00      1.00       343\n",
      "         Low       1.00      1.00      1.00      6866\n",
      "    Moderate       1.00      1.00      1.00      2485\n",
      "\n",
      "    accuracy                           1.00      9694\n",
      "   macro avg       1.00      1.00      1.00      9694\n",
      "weighted avg       1.00      1.00      1.00      9694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuración de validación temporal\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Ordenar por tiempo desde baseline\n",
    "df_clean = df_clean.sort_values('DAYS_SINCE_BASELINE')\n",
    "\n",
    "# Identificar columnas categóricas\n",
    "categorical_cols = df_clean.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remover la columna objetivo de las listas\n",
    "if 'risk_category' in categorical_cols:\n",
    "    categorical_cols.remove('risk_category')\n",
    "if 'risk_category' in numeric_cols:\n",
    "    numeric_cols.remove('risk_category')\n",
    "\n",
    "# Crear transformador de columnas\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Crear pipeline completo\n",
    "gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', HistGradientBoostingClassifier(\n",
    "        loss='log_loss',\n",
    "        max_depth=4,\n",
    "        max_iter=150,\n",
    "        random_state=42,\n",
    "        class_weight={0: 8.0, 1: 1.0, 2: 3.0}\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Usar RID como identificador de paciente\n",
    "X = df_clean.drop(columns=['risk_category'])\n",
    "y = df_clean['risk_category']\n",
    "\n",
    "# Split temporal basado en tiempo transcurrido\n",
    "split_index = int(len(df_clean) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]\n",
    "\n",
    "# Configurar validación cruzada temporal\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Bucle de entrenamiento temporal\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    gbm.fit(X_train_fold, y_train_fold)\n",
    "    y_pred = gbm.predict(X_val_fold)\n",
    "    \n",
    "    # Calcular métricas específicas para High Risk\n",
    "    high_risk_mask = (y_val_fold == 0)\n",
    "    if sum(high_risk_mask) > 0:\n",
    "        recall_high_risk = recall_score(\n",
    "            y_val_fold, \n",
    "            y_pred, \n",
    "            labels=[0],\n",
    "            average=None\n",
    "        )[0]\n",
    "        print(f\"Recall High Risk: {recall_high_risk:.4f}\")\n",
    "\n",
    "# Entrenar modelo final con todos los datos de train\n",
    "final_model = gbm.fit(X_train, y_train)\n",
    "\n",
    "# Evaluación en conjunto de prueba temporal\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "print(\"\\n🔍 Rendimiento en Test Temporal:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7008a2b0-0ecb-484c-891c-ea6f176629ac",
   "metadata": {},
   "source": [
    "## Recopilar métricas de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8484219-c09e-479d-a366-f6b681d99463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recopilar métricas de rendimiento\n",
    "performance_comparison = {}\n",
    "for name, metrics in results.items():\n",
    "    performance_comparison[name] = metrics['f1_weighted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5674c6-5f2d-43b4-b584-8cc00bfa4583",
   "metadata": {},
   "source": [
    "## Ensamblaje de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58258982-f286-4787-8581-6765de0afeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Creando modelos ensemble...\n",
      " Evaluando Voting Classifier...\n",
      " Evaluando Stacking Classifier...\n",
      " Entrenando modelos finales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/03 21:44:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/03 21:46:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/07/03 21:46:33 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/03 21:46:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ensembles creados y registrados!\n",
      "Voting F1: 0.9979\n",
      "Stacking F1: 0.9997\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"ensemble_classification\"):\n",
    "    mlflow.set_tag(\"model_family\", \"ensemble\")\n",
    "    mlflow.set_tag(\"model_type\", \"classification\")\n",
    "    \n",
    "    # 0. Reconstruir datos COMPLETOS desde cero\n",
    "    X_train, X_test, y_train, y_test = classification_pipeline.prepare_data(df_clean)\n",
    "    X_full = pd.concat([X_train, X_test])\n",
    "    y_full = pd.concat([y_train, y_test])\n",
    "    \n",
    "    # 1. Definir best_models usando los modelos entrenados\n",
    "    best_models = {\n",
    "        'logistic_regression': trained_models['logistic_regression'],\n",
    "        'random_forest': trained_models['random_forest'],\n",
    "        'gradient_boosting': trained_models['gradient_boosting'],\n",
    "        'svm': trained_models['svm']\n",
    "    }\n",
    "    \n",
    "    print(\"🚀 Creando modelos ensemble...\")\n",
    "    \n",
    "    # 2. Inicializar ensemble\n",
    "    ensemble = AlzheimerEnsemble()\n",
    "    \n",
    "    # 3. Crear ensembles personalizados\n",
    "    voting_clf = ensemble.create_custom_voting_ensemble(best_models)\n",
    "    stacking_clf = ensemble.create_custom_stacking_ensemble(best_models)\n",
    "    \n",
    "    # 4. Evaluar con cross_validate (para métricas consistentes)\n",
    "    from sklearn.model_selection import cross_validate\n",
    "    \n",
    "    print(\" Evaluando Voting Classifier...\")\n",
    "    voting_cv = cross_validate(\n",
    "        voting_clf, X_full, y_full,\n",
    "        cv=3,  # Reducido para mayor velocidad\n",
    "        scoring=['f1_weighted', 'accuracy'],\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    print(\" Evaluando Stacking Classifier...\")\n",
    "    stacking_cv = cross_validate(\n",
    "        stacking_clf, X_full, y_full,\n",
    "        cv=3,  # Reducido para mayor velocidad\n",
    "        scoring=['f1_weighted', 'accuracy'],\n",
    "        n_jobs=1\n",
    "    )\n",
    "    \n",
    "    # 5. Entrenar modelos finales (con todos los datos)\n",
    "    print(\" Entrenando modelos finales...\")\n",
    "    voting_clf.fit(X_full, y_full)\n",
    "    stacking_clf.fit(X_full, y_full)\n",
    "    \n",
    "    # 6. Registrar resultados\n",
    "    mlflow.log_metrics({\n",
    "        'voting_f1_weighted': voting_cv['test_f1_weighted'].mean(),\n",
    "        'voting_accuracy': voting_cv['test_accuracy'].mean(),\n",
    "        'stacking_f1_weighted': stacking_cv['test_f1_weighted'].mean(),\n",
    "        'stacking_accuracy': stacking_cv['test_accuracy'].mean()\n",
    "    })\n",
    "    \n",
    "    mlflow.sklearn.log_model(voting_clf, \"voting_ensemble\")\n",
    "    mlflow.sklearn.log_model(stacking_clf, \"stacking_ensemble\")\n",
    "    \n",
    "    print(f\"\"\" Ensembles creados y registrados!\n",
    "Voting F1: {voting_cv['test_f1_weighted'].mean():.4f}\n",
    "Stacking F1: {stacking_cv['test_f1_weighted'].mean():.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08350112-7c71-4cc6-a125-dbd9323dd139",
   "metadata": {},
   "source": [
    "4009 segundos de ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44e100c7-1593-4110-8bd5-b015f288e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de clases:\n",
      "risk_category\n",
      "1    0.464264\n",
      "2    0.461045\n",
      "0    0.074692\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Reporte Voting Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      3620\n",
      "           1       1.00      1.00      1.00     22501\n",
      "           2       1.00      1.00      1.00     22345\n",
      "\n",
      "    accuracy                           1.00     48466\n",
      "   macro avg       1.00      1.00      1.00     48466\n",
      "weighted avg       1.00      1.00      1.00     48466\n",
      "\n",
      "\n",
      "Reporte Stacking Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3620\n",
      "           1       1.00      1.00      1.00     22501\n",
      "           2       1.00      1.00      1.00     22345\n",
      "\n",
      "    accuracy                           1.00     48466\n",
      "   macro avg       1.00      1.00      1.00     48466\n",
      "weighted avg       1.00      1.00      1.00     48466\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Distribución de clases\n",
    "print(\"Distribución de clases:\")\n",
    "print(pd.Series(y_full).value_counts(normalize=True))\n",
    "\n",
    "# 2. Rendimiento por clase\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"\\nReporte Voting Classifier:\")\n",
    "print(classification_report(y_full, voting_clf.predict(X_full)))\n",
    "\n",
    "print(\"\\nReporte Stacking Classifier:\")\n",
    "print(classification_report(y_full, stacking_clf.predict(X_full)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4d046-24fb-473e-89e1-de06a6255086",
   "metadata": {},
   "source": [
    "## Paso 4: Evaluación Clínica de Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4edc0b3-0b26-42cd-9029-0157823d43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo extraído del pipeline: HistGradientBoostingClassifier\n",
      "🔧 Configuración para baja memoria: 30 muestras\n",
      "📊 Muestra seleccionada: 30 instancias\n",
      "Distribución de clases: {0.0: 10, 1.0: 10, 2.0: 10}\n",
      "\n",
      "🔍 Casos High Risk en muestra (10):\n",
      "\n",
      "Caso 1:\n",
      "- ABETA42: nan\n",
      "- PTAU: Normal\n",
      "- CDRSB: nan\n",
      "- Edad: nan SD\n",
      "\n",
      "Caso 2:\n",
      "- ABETA42: 1.0\n",
      "- PTAU: Normal\n",
      "- CDRSB: 1.5\n",
      "- Edad: 0.10 SD\n",
      "\n",
      "Caso 3:\n",
      "- ABETA42: nan\n",
      "- PTAU: Normal\n",
      "- CDRSB: nan\n",
      "- Edad: nan SD\n",
      "⚠️ Error en validación clínica: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n",
      "\n",
      "🔝 Método alternativo: Importancia de Características del Modelo\n",
      "⚠️ El modelo no tiene atributo 'feature_importances_'\n",
      "\n",
      "✅ Análisis completado con éxito!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. Extraer el modelo final del pipeline\n",
    "if isinstance(final_model, Pipeline):\n",
    "    model_for_shap = final_model.named_steps[list(final_model.named_steps.keys())[-1]]\n",
    "    print(f\"✅ Modelo extraído del pipeline: {type(model_for_shap).__name__}\")\n",
    "else:\n",
    "    model_for_shap = final_model\n",
    "\n",
    "# 2. Configuración segura para baja memoria\n",
    "SAFE_SAMPLE_SIZE = 30\n",
    "plt.switch_backend('Agg')  # Backend no interactivo\n",
    "\n",
    "print(f\"🔧 Configuración para baja memoria: {SAFE_SAMPLE_SIZE} muestras\")\n",
    "\n",
    "# 3. Solución definitiva para muestreo con índices alineados\n",
    "def get_aligned_sample(X, y, sample_size):\n",
    "    # Crear DataFrame combinado para garantizar alineación\n",
    "    combined = pd.concat([X, y], axis=1)\n",
    "    combined.columns = list(X.columns) + ['target']\n",
    "    \n",
    "    # Filtrar solo filas completas\n",
    "    combined = combined.dropna(subset=['target'])\n",
    "    \n",
    "    # Muestreo estratificado\n",
    "    sample = combined.groupby('target', group_keys=False).apply(lambda x: x.sample(min(len(x), max(1, sample_size // len(combined['target'].unique())))))\n",
    "    \n",
    "    # Si aún es demasiado grande, tomar muestra aleatoria\n",
    "    if len(sample) > sample_size:\n",
    "        sample = sample.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Separar en X e y\n",
    "    X_sample = sample.drop(columns='target')\n",
    "    y_sample = sample['target']\n",
    "    \n",
    "    return X_sample, y_sample\n",
    "\n",
    "# 4. Aplicar muestreo seguro con alineación garantizada\n",
    "try:\n",
    "    X_test_sample, y_test_sample = get_aligned_sample(X_test, y_test, SAFE_SAMPLE_SIZE)\n",
    "    print(f\"📊 Muestra seleccionada: {len(X_test_sample)} instancias\")\n",
    "    print(f\"Distribución de clases: {y_test_sample.value_counts().to_dict()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error en muestreo: {str(e)}\")\n",
    "    print(\"🔁 Usando muestreo simple como alternativa de emergencia\")\n",
    "    \n",
    "    # Muestreo de emergencia: tomar las primeras n filas\n",
    "    X_test_sample = X_test.head(SAFE_SAMPLE_SIZE)\n",
    "    y_test_sample = y_test.head(SAFE_SAMPLE_SIZE)\n",
    "\n",
    "# 5. Análisis de casos High Risk\n",
    "clinical_features = [\n",
    "    'ROCHE_ABETA42_NORMAL',\n",
    "    'PTAU_pathological',\n",
    "    'CDRSB',\n",
    "    'APOE_e4_carrier',\n",
    "    'age_standardized'\n",
    "]\n",
    "\n",
    "if 0 in y_test_sample.values:\n",
    "    high_risk_mask = (y_test_sample == 0)\n",
    "    high_risk_sample = X_test_sample.loc[high_risk_mask]\n",
    "    \n",
    "    print(f\"\\n🔍 Casos High Risk en muestra ({len(high_risk_sample)}):\")\n",
    "    \n",
    "    for i in range(min(3, len(high_risk_sample))):\n",
    "        row = high_risk_sample.iloc[i]\n",
    "        print(f\"\\nCaso {i+1}:\")\n",
    "        print(f\"- ABETA42: {row.get('ROCHE_ABETA42_NORMAL', 'N/A')}\")\n",
    "        print(f\"- PTAU: {'Patológico' if row.get('PTAU_pathological', 0) == 1 else 'Normal'}\")\n",
    "        print(f\"- CDRSB: {row.get('CDRSB', 'N/A')}\")\n",
    "        print(f\"- Edad: {row.get('age_standardized', 'N/A'):.2f} SD\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No se encontraron casos High Risk en la muestra\")\n",
    "\n",
    "# 6. Validación con subpoblaciones clínicas\n",
    "try:\n",
    "    if 'ROCHE_ABETA42_NORMAL' in X_test.columns and 'PTAU_pathological' in X_test.columns:\n",
    "        bio_mask = X_test['ROCHE_ABETA42_NORMAL'].notna() & X_test['PTAU_pathological'].notna()\n",
    "        \n",
    "        if bio_mask.sum() > 0:\n",
    "            y_bio_test = y_test[bio_mask]\n",
    "            X_bio_test = X_test[bio_mask]\n",
    "            y_bio_pred = final_model.predict(X_bio_test)\n",
    "            \n",
    "            print(\"\\n🧪 Rendimiento en Pacientes con Biomarcadores Completos:\")\n",
    "            print(classification_report(y_bio_test, y_bio_pred))\n",
    "        else:\n",
    "            print(\"\\n⚠️ No se encontraron pacientes con biomarcadores completos\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Biomarcadores no disponibles para validación\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error en validación clínica: {str(e)}\")\n",
    "\n",
    "# 7. Análisis de importancia alternativo\n",
    "print(\"\\n🔝 Método alternativo: Importancia de Características del Modelo\")\n",
    "\n",
    "if hasattr(model_for_shap, 'feature_importances_'):\n",
    "    # Obtener nombres de características (manejar diferentes estructuras de pipeline)\n",
    "    if hasattr(X_test, 'columns'):\n",
    "        feature_names = X_test.columns.tolist()\n",
    "    else:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X_test.shape[1])]\n",
    "    \n",
    "    feat_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': model_for_shap.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    print(feat_importance)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    feat_importance.to_csv('../reports/model_results/model_feature_importance.csv', index=False)\n",
    "    \n",
    "    # Visualización simple\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feat_importance['Feature'], feat_importance['Importance'], color='skyblue')\n",
    "    plt.title('Top 10 Features más Importantes')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/model_feature_importance.png', dpi=150)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"⚠️ El modelo no tiene atributo 'feature_importances_'\")\n",
    "\n",
    "print(\"\\n✅ Análisis completado con éxito!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c547cd6-9fdd-4ef0-a37b-cc745a56e301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelo extraído del pipeline: HistGradientBoostingClassifier\n",
      "⚠️ Error preparando muestra SHAP: '[6739, 3709, 6737, 3225] not in index'\n",
      "\n",
      "🔍 Casos High Risk en muestra (10):\n",
      "\n",
      "Caso 1:\n",
      "- ABETA42: nan\n",
      "- PTAU: Normal\n",
      "- CDRSB: nan\n",
      "- Edad: nan SD\n",
      "\n",
      "Caso 2:\n",
      "- ABETA42: 1.0\n",
      "- PTAU: Normal\n",
      "- CDRSB: 1.5\n",
      "- Edad: 0.10 SD\n",
      "\n",
      "Caso 3:\n",
      "- ABETA42: nan\n",
      "- PTAU: Normal\n",
      "- CDRSB: nan\n",
      "- Edad: nan SD\n",
      "⚠️ Error en validación clínica: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n",
      "\n",
      "🔝 Método alternativo: Importancia de Características del Modelo\n",
      "⚠️ El modelo no tiene atributo 'feature_importances_'\n",
      "\n",
      "✅ Análisis completado con éxito!\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import gc  # Para gestión de memoria\n",
    "import random\n",
    "\n",
    "# 1. Extraer el modelo final del pipeline\n",
    "if isinstance(final_model, Pipeline):\n",
    "    model_for_shap = final_model.named_steps[list(final_model.named_steps.keys())[-1]]\n",
    "    print(f\"✅ Modelo extraído del pipeline: {type(model_for_shap).__name__}\")\n",
    "else:\n",
    "    model_for_shap = final_model\n",
    "\n",
    "# 2. Configuración segura para SHAP en baja memoria\n",
    "SHAP_SAMPLE_SIZE = 5  # Muy pequeño pero significativo\n",
    "plt.switch_backend('Agg')  # Backend no interactivo\n",
    "clinical_features = [\n",
    "    'ROCHE_ABETA42_NORMAL',\n",
    "    'PTAU_pathological',\n",
    "    'CDRSB',\n",
    "    'APOE_e4_carrier',\n",
    "    'age_standardized'\n",
    "]\n",
    "\n",
    "# 3. Función para análisis SHAP seguro\n",
    "def run_safe_shap_analysis(model, X_sample, features):\n",
    "    try:\n",
    "        import shap\n",
    "        # Liberar memoria antes de SHAP\n",
    "        gc.collect()\n",
    "        \n",
    "        # Crear explainer con configuración ligera\n",
    "        explainer = shap.TreeExplainer(model, feature_perturbation=\"interventional\")\n",
    "        \n",
    "        # Calcular SHAP values solo para las features clínicas\n",
    "        shap_values = explainer.shap_values(X_sample[features])\n",
    "        \n",
    "        # Visualización mínima\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_sample[features],\n",
    "            feature_names=features,\n",
    "            plot_type=\"dot\",\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(\"Importancia SHAP para Características Clínicas\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/figures/safe_shap_summary.png', dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"✅ SHAP ejecutado exitosamente con 5 muestras\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ SHAP falló: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# 4. Seleccionar muestra específica para SHAP\n",
    "try:\n",
    "    # Obtener índices de pacientes High Risk\n",
    "    high_risk_indices = y_test[y_test == 0].index.tolist()\n",
    "    \n",
    "    # Seleccionar máximo 5 casos High Risk\n",
    "    if len(high_risk_indices) > 0:\n",
    "        shap_sample_indices = random.sample(high_risk_indices, min(SHAP_SAMPLE_SIZE, len(high_risk_indices)))\n",
    "        X_shap_sample = X_test.loc[shap_sample_indices][clinical_features]\n",
    "        \n",
    "        print(f\"🔍 Ejecutando SHAP para {len(X_shap_sample)} casos High Risk...\")\n",
    "        shap_success = run_safe_shap_analysis(model_for_shap, X_shap_sample, clinical_features)\n",
    "        \n",
    "        if shap_success:\n",
    "            print(\"📊 Resultados SHAP guardados en: '../reports/figures/safe_shap_summary.png'\")\n",
    "        else:\n",
    "            print(\"🚫 Continuando sin resultados SHAP\")\n",
    "    else:\n",
    "        print(\"⚠️ No hay casos High Risk para análisis SHAP\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error preparando muestra SHAP: {str(e)}\")\n",
    "\n",
    "# 5. Análisis de casos High Risk\n",
    "clinical_features = [\n",
    "    'ROCHE_ABETA42_NORMAL',\n",
    "    'PTAU_pathological',\n",
    "    'CDRSB',\n",
    "    'APOE_e4_carrier',\n",
    "    'age_standardized'\n",
    "]\n",
    "\n",
    "if 0 in y_test_sample.values:\n",
    "    high_risk_mask = (y_test_sample == 0)\n",
    "    high_risk_sample = X_test_sample.loc[high_risk_mask]\n",
    "    \n",
    "    print(f\"\\n🔍 Casos High Risk en muestra ({len(high_risk_sample)}):\")\n",
    "    \n",
    "    for i in range(min(3, len(high_risk_sample))):\n",
    "        row = high_risk_sample.iloc[i]\n",
    "        print(f\"\\nCaso {i+1}:\")\n",
    "        print(f\"- ABETA42: {row.get('ROCHE_ABETA42_NORMAL', 'N/A')}\")\n",
    "        print(f\"- PTAU: {'Patológico' if row.get('PTAU_pathological', 0) == 1 else 'Normal'}\")\n",
    "        print(f\"- CDRSB: {row.get('CDRSB', 'N/A')}\")\n",
    "        print(f\"- Edad: {row.get('age_standardized', 'N/A'):.2f} SD\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No se encontraron casos High Risk en la muestra\")\n",
    "\n",
    "# 6. Validación con subpoblaciones clínicas\n",
    "try:\n",
    "    if 'ROCHE_ABETA42_NORMAL' in X_test.columns and 'PTAU_pathological' in X_test.columns:\n",
    "        bio_mask = X_test['ROCHE_ABETA42_NORMAL'].notna() & X_test['PTAU_pathological'].notna()\n",
    "        \n",
    "        if bio_mask.sum() > 0:\n",
    "            y_bio_test = y_test[bio_mask]\n",
    "            X_bio_test = X_test[bio_mask]\n",
    "            y_bio_pred = final_model.predict(X_bio_test)\n",
    "            \n",
    "            print(\"\\n🧪 Rendimiento en Pacientes con Biomarcadores Completos:\")\n",
    "            print(classification_report(y_bio_test, y_bio_pred))\n",
    "        else:\n",
    "            print(\"\\n⚠️ No se encontraron pacientes con biomarcadores completos\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Biomarcadores no disponibles para validación\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error en validación clínica: {str(e)}\")\n",
    "\n",
    "# 7. Análisis de importancia alternativo\n",
    "print(\"\\n🔝 Método alternativo: Importancia de Características del Modelo\")\n",
    "\n",
    "if hasattr(model_for_shap, 'feature_importances_'):\n",
    "    # Obtener nombres de características (manejar diferentes estructuras de pipeline)\n",
    "    if hasattr(X_test, 'columns'):\n",
    "        feature_names = X_test.columns.tolist()\n",
    "    else:\n",
    "        feature_names = [f\"feature_{i}\" for i in range(X_test.shape[1])]\n",
    "    \n",
    "    feat_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': model_for_shap.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False).head(10)\n",
    "    \n",
    "    print(feat_importance)\n",
    "    \n",
    "    # Guardar resultados\n",
    "    feat_importance.to_csv('../reports/model_results/model_feature_importance.csv', index=False)\n",
    "    \n",
    "    # Visualización simple\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feat_importance['Feature'], feat_importance['Importance'], color='skyblue')\n",
    "    plt.title('Top 10 Features más Importantes')\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../reports/figures/model_feature_importance.png', dpi=150)\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\"⚠️ El modelo no tiene atributo 'feature_importances_'\")\n",
    "\n",
    "print(\"\\n✅ Análisis completado con éxito!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6804bd6-be0d-44bb-bb05-9d1dc2371052",
   "metadata": {},
   "source": [
    "## Comparación Final de Modelos (agregando los ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab6eab4-c8e0-4c37-aacb-1d58c20407f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "📊 COMPARACIÓN FINAL DE MODELOS\n",
      "============================================================\n",
      "              Model  F1_Score_Weighted\n",
      "  gradient_boosting           0.999794\n",
      "      random_forest           0.995879\n",
      "logistic_regression           0.985013\n",
      "                svm           0.776901\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 COMPARACIÓN FINAL DE MODELOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Crear DataFrame comparativo\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'Model': model, 'F1_Score_Weighted': score} \n",
    "    for model, score in performance_comparison.items()\n",
    "]).sort_values('F1_Score_Weighted', ascending=False)\n",
    "\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualización de comparación\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(comparison_df['Model'], comparison_df['F1_Score_Weighted'], \n",
    "               color='lightcoral', alpha=0.8)\n",
    "plt.title('Comparación de Rendimiento - Modelos de Clasificación')\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('F1-Score Weighted')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, score in zip(bars, comparison_df['F1_Score_Weighted']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{score:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "# Guardar imagen\n",
    "plt.savefig('../reports/figures/model_comparison_classification.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6525ba-a9e2-4f48-8c23-37f231d62da5",
   "metadata": {},
   "source": [
    "## Resumen final y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b284bf1-eae6-4718-b99d-f53704508621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎯 RESUMEN FINAL - MODELOS DE CLASIFICACIÓN\n",
      "============================================================\n",
      "🏆 Mejor modelo: gradient_boosting\n",
      "📊 F1-Score Weighted: 0.9995\n",
      "📈 Mejora sobre baseline: 3.1%\n",
      "⚖️ Desbalanceamiento de clases: 6.22\n",
      "🧠 Features utilizadas: 184\n",
      "🔢 Muestras totales: 48466\n",
      "\n",
      "💡 RECOMENDACIONES BASADAS EN RESULTADOS:\n",
      "🔍 ¡Resultados excepcionales! Verificar:\n",
      "   - Posible data leakage (revisar 'composite_risk_score')\n",
      "   - Calidad de los datos (¿valores constantes o duplicados?)\n",
      "\n",
      "⚖️ ALERTA: Desbalanceo significativo (ratio 6.2:1)\n",
      "   - Técnicas recomendadas:\n",
      "     * SMOTE para oversampling\n",
      "     * Class weighting en modelos\n",
      "     * Métricas adicionales (Precision-Recall Curve)\n",
      "\n",
      "🔜 PRÓXIMOS PASOS RECOMENDADOS:\n",
      "1. Validación en conjunto de prueba independiente\n",
      "2. Análisis de errores (matriz de confusión)\n",
      "3. Revisión clínica de features importantes\n",
      "4. Documentación técnica completa\n"
     ]
    }
   ],
   "source": [
    "# 1. Definir las variables necesarias si no existen\n",
    "if 'best_model' not in locals():\n",
    "    # Obtener el mejor modelo de los resultados del pipeline si existe\n",
    "    if 'pipeline_results' in locals():\n",
    "        best_model = pipeline_results.get('best_model', '')\n",
    "        best_score = pipeline_results['results'].get(best_model, {}).get('f1_weighted', 0)\n",
    "    else:\n",
    "        # Calcular el mejor modelo desde performance_comparison\n",
    "        if 'performance_comparison' in locals():\n",
    "            best_model = max(performance_comparison.items(), key=lambda x: x[1])[0]\n",
    "            best_score = performance_comparison[best_model]\n",
    "        else:\n",
    "            # Usar el primer modelo de best_models como fallback\n",
    "            best_model = next(iter(best_models)) if best_models else 'N/A'\n",
    "            best_score = 0\n",
    "\n",
    "# 2. Calcular el baseline si no existe\n",
    "if 'lr_performance' not in locals():\n",
    "    if 'logistic_regression' in performance_comparison:\n",
    "        lr_performance = performance_comparison['logistic_regression']\n",
    "    elif 'logistic_regression' in trained_models:\n",
    "        from sklearn.metrics import f1_score\n",
    "        lr_pred = trained_models['logistic_regression'].predict(X_full)\n",
    "        lr_performance = f1_score(y_full, lr_pred, average='weighted')\n",
    "    else:\n",
    "        lr_performance = 0\n",
    "\n",
    "# 3. Calcular imbalance_ratio si no existe\n",
    "if 'imbalance_ratio' not in locals():\n",
    "    class_counts = pd.Series(y_full).value_counts()\n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "\n",
    "# 4. Mostrar resumen mejorado\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 RESUMEN FINAL - MODELOS DE CLASIFICACIÓN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"🏆 Mejor modelo: {best_model}\")\n",
    "print(f\"📊 F1-Score Weighted: {best_score:.4f}\")\n",
    "\n",
    "if lr_performance > 0:\n",
    "    improvement = (best_score - lr_performance) / lr_performance * 100\n",
    "    print(f\"📈 Mejora sobre baseline: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"⚖️ Desbalanceamiento de clases: {imbalance_ratio:.2f}\")\n",
    "print(f\"🧠 Features utilizadas: {len(feature_cols)}\")\n",
    "print(f\"🔢 Muestras totales: {len(X_full)}\")\n",
    "\n",
    "# 5. Recomendaciones técnicas\n",
    "print(\"\\n💡 RECOMENDACIONES BASADAS EN RESULTADOS:\")\n",
    "if best_score > 0.99:\n",
    "    print(\"🔍 ¡Resultados excepcionales! Verificar:\")\n",
    "    print(\"   - Posible data leakage (revisar 'composite_risk_score')\")\n",
    "    print(\"   - Calidad de los datos (¿valores constantes o duplicados?)\")\n",
    "elif best_score > 0.85:\n",
    "    print(\"✅ Excelente rendimiento. Acciones:\")\n",
    "    print(\"   - Implementar sistema de monitoreo continuo\")\n",
    "    print(\"   - Documentar importancia de features\")\n",
    "else:\n",
    "    print(\"🛠 Oportunidades de mejora:\")\n",
    "    print(\"   - Optimizar hiperparámetros\")\n",
    "    print(\"   - Considerar ingeniería de features adicional\")\n",
    "\n",
    "if imbalance_ratio > 3:\n",
    "    print(f\"\\n⚖️ ALERTA: Desbalanceo significativo (ratio {imbalance_ratio:.1f}:1)\")\n",
    "    print(\"   - Técnicas recomendadas:\")\n",
    "    print(\"     * SMOTE para oversampling\")\n",
    "    print(\"     * Class weighting en modelos\")\n",
    "    print(\"     * Métricas adicionales (Precision-Recall Curve)\")\n",
    "\n",
    "# 6. Próximos pasos\n",
    "print(\"\\n🔜 PRÓXIMOS PASOS RECOMENDADOS:\")\n",
    "print(\"1. Validación en conjunto de prueba independiente\")\n",
    "print(\"2. Análisis de errores (matriz de confusión)\")\n",
    "print(\"3. Revisión clínica de features importantes\")\n",
    "print(\"4. Documentación técnica completa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc956bf4-f1f0-4459-a108-b25b5866412f",
   "metadata": {},
   "source": [
    "## Guardado de Archivos Importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d1d0b5-b90c-4238-8393-7ea842323335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Métricas guardadas en:\n",
      "📂 E:\\usuarios\\alumno\\Escritorio\\Alzheimer-Multimodal-Monitoring\\reports\\model_results\\classification_summary.csv\n",
      "\n",
      "Contenido guardado:\n",
      "best_model                  gradient_boosting\n",
      "best_f1_score                        0.999794\n",
      "baseline_f1_score                    0.985013\n",
      "improvement_percentage               1.500589\n",
      "models_trained                              4\n",
      "imbalance_ratio                      6.215746\n",
      "training_samples                        48466\n",
      "features_used                             163\n",
      "timestamp                 2025-07-04 10:24:27\n"
     ]
    }
   ],
   "source": [
    "# SOLUCIÓN ROBUSTA PARA GUARDAR MÉTRICAS\n",
    "\n",
    "# 1. Definir las variables necesarias si no existen\n",
    "if 'best_model' not in locals():\n",
    "    # Obtener el mejor modelo de los resultados del pipeline si existe\n",
    "    if 'pipeline_results' in locals():\n",
    "        best_model = pipeline_results.get('best_model', '')\n",
    "        best_score = pipeline_results['results'].get(best_model, {}).get('f1_weighted', 0)\n",
    "    else:\n",
    "        # Calcular el mejor modelo desde performance_comparison\n",
    "        if 'performance_comparison' in locals():\n",
    "            best_model = max(performance_comparison.items(), key=lambda x: x[1])[0]\n",
    "            best_score = performance_comparison[best_model]\n",
    "        else:\n",
    "            # Usar el primer modelo de best_models como fallback\n",
    "            best_model = next(iter(trained_models)) if 'trained_models' in locals() else 'N/A'\n",
    "            best_score = 0\n",
    "\n",
    "# 2. Calcular el baseline si no existe\n",
    "if 'lr_performance' not in locals():\n",
    "    if 'logistic_regression' in performance_comparison:\n",
    "        lr_performance = performance_comparison['logistic_regression']\n",
    "    elif 'logistic_regression' in trained_models:\n",
    "        from sklearn.metrics import f1_score\n",
    "        lr_pred = trained_models['logistic_regression'].predict(X_full)\n",
    "        lr_performance = f1_score(y_full, lr_pred, average='weighted')\n",
    "    else:\n",
    "        lr_performance = 0\n",
    "\n",
    "# 3. Calcular imbalance_ratio si no existe\n",
    "if 'imbalance_ratio' not in locals():\n",
    "    if 'y_full' in locals():\n",
    "        class_counts = pd.Series(y_full).value_counts()\n",
    "    elif 'y_test' in locals():\n",
    "        class_counts = pd.Series(y_test).value_counts()\n",
    "    else:\n",
    "        class_counts = pd.Series(y_train).value_counts()\n",
    "    \n",
    "    imbalance_ratio = class_counts.max() / class_counts.min()\n",
    "\n",
    "# 4. Obtener feature_cols si no existe\n",
    "if 'feature_cols' not in locals():\n",
    "    if 'X_train' in locals():\n",
    "        feature_cols = X_train.columns.tolist()\n",
    "    elif 'X_full' in locals():\n",
    "        feature_cols = X_full.columns.tolist()\n",
    "    else:\n",
    "        feature_cols = []\n",
    "\n",
    "# 5. Obtener número de muestras\n",
    "if 'X_full' in locals():\n",
    "    training_samples = len(X_full)\n",
    "elif 'X_train' in locals() and 'X_test' in locals():\n",
    "    training_samples = len(X_train) + len(X_test)\n",
    "else:\n",
    "    training_samples = 0\n",
    "\n",
    "# 6. Guardar métricas finales\n",
    "final_metrics = {\n",
    "    'best_model': best_model,\n",
    "    'best_f1_score': best_score,\n",
    "    'baseline_f1_score': lr_performance,\n",
    "    'improvement_percentage': (best_score - lr_performance) / lr_performance * 100 if lr_performance > 0 else 0,\n",
    "    'models_trained': len(performance_comparison) if 'performance_comparison' in locals() else 0,\n",
    "    'imbalance_ratio': imbalance_ratio,\n",
    "    'training_samples': training_samples,\n",
    "    'features_used': len(feature_cols),\n",
    "    'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "\n",
    "# 7. Crear directorio si no existe\n",
    "import os\n",
    "os.makedirs('../reports/model_results', exist_ok=True)\n",
    "\n",
    "# 8. Guardar en archivo\n",
    "filepath = '../reports/model_results/classification_summary.csv'\n",
    "pd.Series(final_metrics).to_csv(filepath, header=['Value'], index_label='Metric')\n",
    "\n",
    "# 9. Mostrar resultados\n",
    "print(\"✅ Métricas guardadas en:\")\n",
    "print(f\"📂 {os.path.abspath(filepath)}\")\n",
    "print(\"\\nContenido guardado:\")\n",
    "print(pd.Series(final_metrics).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea49671-6862-465d-b167-7d9376375a4d",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd92a04-76e6-4e89-a049-50109034d6e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683824d-8d44-4159-bfde-39748ef333a3",
   "metadata": {},
   "source": [
    "__Abraham Tartalos__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583592d-cf95-40a8-93d1-259a27e3533f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (Alzheimer)",
   "language": "python",
   "name": "alzheimer-env-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
